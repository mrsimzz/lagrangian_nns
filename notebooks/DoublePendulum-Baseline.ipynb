{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np # get rid of this eventually\n",
    "import argparse\n",
    "from jax import jit\n",
    "from jax.experimental.ode import odeint\n",
    "from functools import partial # reduces arguments to function by making some subset implicit\n",
    "\n",
    "from jax.experimental import stax\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import os, sys, time\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../experiment_dblpend/')\n",
    "\n",
    "from lnn import lagrangian_eom_rk4, lagrangian_eom, unconstrained_eom, raw_lagrangian_eom\n",
    "from data import get_dataset\n",
    "from models import mlp as make_mlp\n",
    "from utils import wrap_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../hyperopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyperparameterSearch import learned_dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyperparameterSearch import extended_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectView(object):\n",
    "    def __init__(self, d): self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_trajectory_analytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from physics import analytical_fn\n",
    "\n",
    "vfnc = jax.jit(jax.vmap(analytical_fn))\n",
    "vget = partial(jax.jit, backend='cpu')(jax.vmap(partial(get_trajectory_analytic, ), (0, None), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.29830917716026306 {'act': [4],\n",
    "# 'batch_size': [27.0], 'dt': [0.09609870774790222],\n",
    "# 'hidden_dim': [596.0], 'l2reg': [0.24927677946969878],\n",
    "# 'layers': [4.0], 'lr': [0.005516656601005163],\n",
    "# 'lr2': [1.897157209816416e-05], 'n_updates': [4.0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's load the best model. To generate more models, see the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded = pkl.load(open('./params_for_loss_0.29429444670677185_nupdates=1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ObjectView({'dataset_size': 200,\n",
    " 'fps': 10,\n",
    " 'samples': 100,\n",
    " 'num_epochs': 80000,\n",
    " 'seed': 0,\n",
    " 'loss': 'l1',\n",
    " 'act': 'relu_relu',\n",
    " 'hidden_dim': 600,\n",
    " 'output_dim': 2,\n",
    " 'layers': 3,\n",
    " 'n_updates': 1,\n",
    " 'lr': 0.001,\n",
    " 'lr2': 2e-05,\n",
    " 'dt': 0.1,\n",
    " 'model': 'gln',\n",
    " 'batch_size': 512,\n",
    " 'l2reg': 5.7e-07,\n",
    "})\n",
    "# args = loaded['args']\n",
    "rng = jax.random.PRNGKey(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.ode import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyperparameterSearch import new_get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfnc = jax.jit(jax.vmap(analytical_fn, 0, 0))\n",
    "vget = partial(jax.jit, backend='cpu')(jax.vmap(partial(get_trajectory_analytic, ), (0, None), 0))\n",
    "minibatch_per = 2000\n",
    "batch = 512\n",
    "\n",
    "@jax.jit\n",
    "def get_derivative_dataset(rng):\n",
    "    # randomly sample inputs\n",
    "\n",
    "    y0 = jnp.concatenate([\n",
    "        jax.random.uniform(rng, (batch*minibatch_per, 2))*2.0*np.pi,\n",
    "        (jax.random.uniform(rng+1, (batch*minibatch_per, 2))-0.5)*10*2\n",
    "    ], axis=1)\n",
    "    \n",
    "    return y0, vfnc(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = None\n",
    "best_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "def baseline_eom(baseline, state, t=None):\n",
    "    q, q_t = jnp.split(state, 2)\n",
    "    q = q % (2*jnp.pi)\n",
    "    q_tt = baseline(q, q_t)\n",
    "    return jnp.concatenate([q_t, q_tt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_random_params, nn_forward_fn = extended_mlp(args)\n",
    "import HyperparameterSearch\n",
    "HyperparameterSearch.nn_forward_fn = nn_forward_fn\n",
    "_, init_params = init_random_params(rng+1, (-1, 4))\n",
    "rng += 1\n",
    "model = (nn_forward_fn, init_params)\n",
    "opt_init, opt_update, get_params = optimizers.adam(args.lr)\n",
    "opt_state = opt_init([[l2/200.0 for l2 in l1] for l1 in init_params])\n",
    "from jax.tree_util import tree_flatten\n",
    "from HyperparameterSearch import make_loss, train\n",
    "from copy import deepcopy as copy\n",
    "# train(args, model, data, rng);\n",
    "from jax.tree_util import tree_flatten\n",
    "\n",
    "@jax.jit\n",
    "def loss(params, batch, l2reg):\n",
    "    state, targets = batch#_rk4\n",
    "    leaves, _ = tree_flatten(params)\n",
    "    l2_norm = sum(jnp.vdot(param, param) for param in leaves)\n",
    "    preds = jax.vmap(\n",
    "        partial(\n",
    "            baseline_eom,\n",
    "            learned_dynamics(params)))(state)\n",
    "    return jnp.sum(jnp.abs(preds - targets)) + l2reg*l2_norm/args.batch_size\n",
    "\n",
    "# @jax.jit\n",
    "# def normalize_param_update(param_update):\n",
    "#     new_params = []\n",
    "#     num_weights = args.hidden_dim**2*3\n",
    "#     gradient_norm = sum([jnp.sum(l2**2)\n",
    "#                          for l1 in param_update\n",
    "#                          for l2 in l1\n",
    "#                          if len(l1) != 0])/num_weights\n",
    "# #     gradient_norm = 1 + \n",
    "#     for l1 in param_update:\n",
    "#         if (len(l1)) == 0: new_params.append(()); continue\n",
    "#         new_l1 = []\n",
    "#         for l2 in l1:\n",
    "#             new_l1.append(\n",
    "#                 l2/gradient_norm\n",
    "#             )\n",
    "\n",
    "#         new_params.append(new_l1)\n",
    "        \n",
    "#     return new_params\n",
    "\n",
    "@jax.jit\n",
    "def update_derivative(i, opt_state, batch, l2reg):\n",
    "    params = get_params(opt_state)\n",
    "    param_update = jax.grad(\n",
    "            lambda *args: loss(*args)/len(batch),\n",
    "            0\n",
    "        )(params, batch, l2reg)\n",
    "#     param_update = normalize_param_update(param_update)\n",
    "    params = get_params(opt_state)\n",
    "    return opt_update(i, param_update, opt_state), params\n",
    "\n",
    "\n",
    "best_small_loss = np.inf\n",
    "(nn_forward_fn, init_params) = model\n",
    "iteration = 0\n",
    "total_epochs = 300\n",
    "minibatch_per = 2000\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "lr = 4e-5 #1e-3\n",
    "import math\n",
    "\n",
    "final_div_factor=1e4\n",
    "\n",
    "#OneCycleLR:\n",
    "@jax.jit\n",
    "def OneCycleLR(pct):\n",
    "    #Rush it:\n",
    "    start = 0.2 #0.2\n",
    "    pct = pct * (1-start) + start\n",
    "    high, low = lr, lr/final_div_factor\n",
    "    \n",
    "    scale = 1.0 - (jnp.cos(2 * jnp.pi * pct) + 1)/2\n",
    "    \n",
    "    return low + (high - low)*scale\n",
    "    \n",
    "from lnn import custom_init\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(\n",
    "    OneCycleLR\n",
    ")\n",
    "\n",
    "# init_params = custom_init(init_params, seed=0)\n",
    "# init_params = \n",
    "opt_state = opt_init(init_params)\n",
    "# opt_state = opt_init(best_params)\n",
    "bad_iterations = 0\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: add identity before inverse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = get_derivative_dataset(rng)[0][:10], get_derivative_dataset(rng)[1][:10]\n",
    "print(batch_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(get_params(opt_state), batch_data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state, params = update_derivative(0.0, opt_state, batch_data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss = np.inf\n",
    "# best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epoch, total_epochs)):\n",
    "    epoch_loss = 0.0\n",
    "    num_samples = 0\n",
    "    all_batch_data = get_derivative_dataset(rng)\n",
    "    for minibatch in range(minibatch_per):\n",
    "        fraction = (epoch + minibatch/minibatch_per)/total_epochs\n",
    "        batch_data = (all_batch_data[0][minibatch*batch:(minibatch+1)*batch], all_batch_data[1][minibatch*batch:(minibatch+1)*batch])\n",
    "        rng += 10\n",
    "        opt_state, params = update_derivative(fraction, opt_state, batch_data, 1e-6)\n",
    "        cur_loss = loss(params, batch_data, 0.0)\n",
    "        epoch_loss += cur_loss\n",
    "        num_samples += batch\n",
    "    closs = epoch_loss/num_samples\n",
    "    print('epoch={} lr={} loss={}'.format(\n",
    "        epoch, OneCycleLR(fraction), closs)\n",
    "         )\n",
    "    if closs < best_loss:\n",
    "        best_loss = closs\n",
    "        best_params = [[copy(jax.device_get(l2)) for l2 in l1] if len(l1) > 0 else () for l1 in params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at distribution of weights to make a better model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = pkl.load(\n",
    "    open('best_dblpendulum_baseline_v5_900epoch.pt', 'rb')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = opt_init(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl.dump(\n",
    "#     best_params,\n",
    "#     open('best_dblpendulum_baseline_v5_900epoch.pt', 'wb')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure the args are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_state = opt_init(loaded['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng+7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seed: [8, 8] looks pretty good! Set args.n_updates=3, and the file params_for_loss_0.29429444670677185_nupdates=1.pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', family='serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_t = 10\n",
    "new_dataset = new_get_dataset(jax.random.PRNGKey(2),\n",
    "                              t_span=[0, max_t],\n",
    "                              fps=10, test_split=1.0,\n",
    "                              unlimited_steps=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = new_dataset['x'][0, :]\n",
    "tall = [jax.device_get(t)]\n",
    "p = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tall = jax.device_get(odeint(\n",
    "    partial(baseline_eom, learned_dynamics(p)),\n",
    "    t,\n",
    "    np.linspace(0, max_t, num=new_dataset['x'].shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def kinetic_energy(state, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
    "    q, q_dot = jnp.split(state, 2)\n",
    "    (t1, t2), (w1, w2) = q, q_dot\n",
    "\n",
    "    T1 = 0.5 * m1 * (l1 * w1)**2\n",
    "    T2 = 0.5 * m2 * ((l1 * w1)**2 + (l2 * w2)**2 + 2 * l1 * l2 * w1 * w2 * jnp.cos(t1 - t2))\n",
    "    T = T1 + T2\n",
    "    return T\n",
    "\n",
    "@jit\n",
    "def potential_energy(state, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
    "    q, q_dot = jnp.split(state, 2)\n",
    "    (t1, t2), (w1, w2) = q, q_dot\n",
    "\n",
    "    y1 = -l1 * jnp.cos(t1)\n",
    "    y2 = y1 - l2 * jnp.cos(t2)\n",
    "    V = m1 * g * y1 + m2 * g * y2\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tall = np.array(tall)\n",
    "plt.plot(new_dataset['x'][:30, 0])#[:100, 0])\n",
    "plt.plot(pred_tall[:30, 0])\n",
    "\n",
    "plt.ylabel(r'$\\theta_1$')\n",
    "plt.xlabel('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errors = []\n",
    "for i in tqdm(range(100)):\n",
    "    max_t = 100\n",
    "    new_dataset = new_get_dataset(jax.random.PRNGKey(i),\n",
    "                                  t_span=[0, max_t],\n",
    "                                  fps=10, test_split=1.0,\n",
    "                                  unlimited_steps=False)\n",
    "    t = new_dataset['x'][0, :]\n",
    "    tall = [jax.device_get(t)]\n",
    "    p = best_params\n",
    "    pred_tall = jax.device_get(odeint(\n",
    "        partial(baseline_eom, learned_dynamics(p)),\n",
    "        t,\n",
    "        np.linspace(0, max_t, num=new_dataset['x'].shape[0]),\n",
    "        ))\n",
    "\n",
    "    total_true_energy = (\n",
    "        jax.vmap(kinetic_energy, 0, 0)(new_dataset['x'][:]) + \\\n",
    "        jax.vmap(potential_energy, 0, 0)(new_dataset['x'][:])\n",
    "    )\n",
    "    total_predicted_energy = (\n",
    "        jax.vmap(kinetic_energy, 0, 0)(pred_tall[:]) + \\\n",
    "        jax.vmap(potential_energy, 0, 0)(pred_tall[:])\n",
    "    )\n",
    "\n",
    "    scale=29.4\n",
    "\n",
    "    # translation = jnp.min(total_true_energy) + 1\n",
    "    # total_true_energy -= translation\n",
    "    # total_predicted_energy -= translation\n",
    "\n",
    "    cur_error = jnp.abs((total_predicted_energy-total_true_energy)[-1])/scale\n",
    "    all_errors.append(cur_error)\n",
    "    \n",
    "    print(i, 'current error', jnp.average(all_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can't run the 44th system. Just freezes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_true_energy = (\n",
    "    jax.vmap(kinetic_energy, 0, 0)(new_dataset['x'][:]) + \\\n",
    "    jax.vmap(potential_energy, 0, 0)(new_dataset['x'][:])\n",
    ")\n",
    "total_predicted_energy = (\n",
    "    jax.vmap(kinetic_energy, 0, 0)(pred_tall[:]) + \\\n",
    "    jax.vmap(potential_energy, 0, 0)(pred_tall[:])\n",
    ")\n",
    "scale=29.4\n",
    "\n",
    "# translation = jnp.min(total_true_energy) + 1\n",
    "# total_true_energy -= translation\n",
    "# total_predicted_energy -= translation\n",
    "\n",
    "plt.plot(\n",
    "    (total_predicted_energy-total_true_energy)/scale\n",
    ")\n",
    "\n",
    "plt.ylabel('Absolute Error in Total Energy/Max Potential Energy')\n",
    "plt.xlabel('Time')\n",
    "plt.ylim(-0.06, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('baseline_dblpend_energy.npy', total_predicted_energy)\n",
    "# np.save('baseline_dblpend_prediction.npy', pred_tall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(int(1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = get_derivative_dataset(rng)[0][:100000], get_derivative_dataset(rng)[1][:100000]\n",
    "print(batch_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(best_params, batch_data, 0.0)/len(batch_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = np.inf\n",
    "best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _i in range(1000):\n",
    "    print('Running', _i)\n",
    "    print('Cur best', str(best_loss))\n",
    "\n",
    "    init_random_params, nn_forward_fn = extended_mlp(args)\n",
    "    import HyperparameterSearch\n",
    "    HyperparameterSearch.nn_forward_fn = nn_forward_fn\n",
    "    _, init_params = init_random_params(rng+1, (-1, 4))\n",
    "    rng += 1\n",
    "    model = (nn_forward_fn, init_params)\n",
    "    opt_init, opt_update, get_params = optimizers.adam(3e-4)##lambda i: jnp.select([i<10000, i>= 10000], [args.lr, args.lr2]))\n",
    "    opt_state = opt_init(init_params)\n",
    "    from jax.tree_util import tree_flatten\n",
    "    from HyperparameterSearch import make_loss, train\n",
    "    loss = make_loss(args)\n",
    "    from copy import deepcopy as copy\n",
    "    train(args, model, data, rng);\n",
    "    from jax.tree_util import tree_flatten\n",
    "\n",
    "    @jax.jit\n",
    "    def update_derivative(i, opt_state, batch, l2reg):\n",
    "        params = get_params(opt_state)\n",
    "        param_update = jax.grad(loss, 0)(params, batch, l2reg)\n",
    "        leaves, _ = tree_flatten(param_update)\n",
    "        infinities = sum((~jnp.isfinite(param)).sum() for param in leaves)\n",
    "\n",
    "        def true_fun(x):\n",
    "            #No introducing NaNs.\n",
    "            return opt_update(i, param_update, opt_state), params\n",
    "\n",
    "        def false_fun(x):\n",
    "            #No introducing NaNs.\n",
    "            return opt_state, params\n",
    "\n",
    "        return jax.lax.cond(infinities==0, 0, true_fun, 0, false_fun)\n",
    "\n",
    "\n",
    "    best_small_loss = np.inf\n",
    "    (nn_forward_fn, init_params) = model\n",
    "    data = {k: jax.device_put(v) for k,v in data.items()}\n",
    "    iteration = 0\n",
    "    train_losses, test_losses = [], []\n",
    "    lr = args.lr\n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(init_params)\n",
    "    bad_iterations = 0\n",
    "    offset = 0\n",
    "    \n",
    "    while iteration < 20000:\n",
    "        iteration += 1\n",
    "        rand_idx = jax.random.randint(rng, (args.batch_size,), 0, len(data['x']))\n",
    "        rng += 1\n",
    "\n",
    "        batch = (data['x'][rand_idx], data['dx'][rand_idx])\n",
    "        opt_state, params = update_derivative(iteration+offset, opt_state, batch, args.l2reg)\n",
    "        small_loss = loss(params, batch, 0.0)\n",
    "\n",
    "        new_small_loss = False\n",
    "        if small_loss < best_small_loss:\n",
    "\n",
    "            best_small_loss = small_loss\n",
    "            new_small_loss = True\n",
    "        \n",
    "        if jnp.isnan(small_loss).sum() or new_small_loss or (iteration % 500 == 0) or (iteration < 1000 and iteration % 100 == 0):\n",
    "            params = get_params(opt_state)\n",
    "            train_loss = loss(params, (data['x'], data['dx']), 0.0)/len(data['x'])\n",
    "            train_losses.append(train_loss)\n",
    "            test_loss = loss(params, (data['test_x'], data['test_dx']), 0.0)/len(data['test_x'])\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            if iteration >= 1000 and test_loss > 1.5:\n",
    "                #Only good seeds allowed!\n",
    "                break\n",
    "\n",
    "            if test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                best_params = copy(params)\n",
    "                bad_iterations = 0\n",
    "                offset += iteration\n",
    "                iteration = 0 #Keep going since this one is so good!\n",
    "\n",
    "            if jnp.isnan(test_loss).sum():\n",
    "                break\n",
    "                lr = lr/2\n",
    "                opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "                opt_state = opt_init(best_params)\n",
    "                bad_iterations = 0\n",
    "\n",
    "            print(f\"iteration={iteration}, train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\")\n",
    "\n",
    "        bad_iterations += 1\n",
    "    \n",
    "    import pickle as pkl\n",
    "    if best_loss < np.inf:\n",
    "        pkl.dump({'params': best_params, 'args': args},\n",
    "             open('params_for_loss_{}_nupdates=1.pkl'.format(best_loss), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(lnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lnn import lagrangian_eom_rk4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(best_params, (data['test_x'], data['test_dx']), 0.0)/len(data['test_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main2",
   "language": "python",
   "name": "main2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
