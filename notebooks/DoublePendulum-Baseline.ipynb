{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit\n",
    "from jax.experimental.ode import odeint\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from jax.example_libraries import optimizers\n",
    "from lnn.hyperopt.HyperparameterSearch import learned_dynamics, extended_mlp\n",
    "from lnn.hyperopt.HyperparameterSearch import new_get_dataset\n",
    "from lnn.experiment_dblpend.data import get_trajectory_analytic\n",
    "from lnn.experiment_dblpend.physics import analytical_fn\n",
    "from lnn.hyperopt import HyperparameterSearch\n",
    "from lnn.hyperopt.HyperparameterSearch import make_loss, train\n",
    "from jax.tree_util import tree_flatten\n",
    "from copy import deepcopy as copy\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectView(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfnc = jax.jit(jax.vmap(analytical_fn))\n",
    "vget = partial(jax.jit, backend=\"cpu\")(\n",
    "    jax.vmap(\n",
    "        partial(\n",
    "            get_trajectory_analytic,\n",
    "        ),\n",
    "        (0, None),\n",
    "        0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.29830917716026306 {'act': [4],\n",
    "# 'batch_size': [27.0], 'dt': [0.09609870774790222],\n",
    "# 'hidden_dim': [596.0], 'l2reg': [0.24927677946969878],\n",
    "# 'layers': [4.0], 'lr': [0.005516656601005163],\n",
    "# 'lr2': [1.897157209816416e-05], 'n_updates': [4.0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's load the best model. To generate more models, see the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded = pkl.load(open('./params_for_loss_0.29429444670677185_nupdates=1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ObjectView(\n",
    "    {\n",
    "        \"dataset_size\": 200,\n",
    "        \"fps\": 10,\n",
    "        \"samples\": 100,\n",
    "        \"num_epochs\": 80000,\n",
    "        \"seed\": 0,\n",
    "        \"loss\": \"l1\",\n",
    "        \"act\": \"relu_relu\",\n",
    "        \"hidden_dim\": 600,\n",
    "        \"output_dim\": 2,\n",
    "        \"layers\": 3,\n",
    "        \"n_updates\": 1,\n",
    "        \"lr\": 0.001,\n",
    "        \"lr2\": 2e-05,\n",
    "        \"dt\": 0.1,\n",
    "        \"model\": \"gln\",\n",
    "        \"batch_size\": 512,\n",
    "        \"l2reg\": 5.7e-07,\n",
    "    }\n",
    ")\n",
    "# args = loaded['args']\n",
    "rng = jax.random.PRNGKey(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfnc = jax.jit(jax.vmap(analytical_fn, 0, 0))\n",
    "vget = partial(jax.jit, backend=\"cpu\")(\n",
    "    jax.vmap(\n",
    "        partial(\n",
    "            get_trajectory_analytic,\n",
    "        ),\n",
    "        (0, None),\n",
    "        0,\n",
    "    )\n",
    ")\n",
    "minibatch_per = 2000\n",
    "batch = 512\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def get_derivative_dataset(rng):\n",
    "    # randomly sample inputs\n",
    "\n",
    "    y0 = jnp.concatenate(\n",
    "        [\n",
    "            jax.random.uniform(rng, (batch * minibatch_per, 2)) * 2.0 * np.pi,\n",
    "            (jax.random.uniform(rng + 1, (batch * minibatch_per, 2)) - 0.5) * 10 * 2,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return y0, vfnc(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = None\n",
    "best_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "def baseline_eom(baseline, state, t=None):\n",
    "    q, q_t = jnp.split(state, 2)\n",
    "    q = q % (2 * jnp.pi)\n",
    "    q_tt = baseline(q, q_t)\n",
    "    return jnp.concatenate([q_t, q_tt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_random_params, nn_forward_fn = extended_mlp(args)\n",
    "\n",
    "\n",
    "HyperparameterSearch.nn_forward_fn = nn_forward_fn\n",
    "_, init_params = init_random_params(rng + 1, (-1, 4))\n",
    "rng += 1\n",
    "model = (nn_forward_fn, init_params)\n",
    "opt_init, opt_update, get_params = optimizers.adam(args.lr)\n",
    "opt_state = opt_init([[l2 / 200.0 for l2 in l1] for l1 in init_params])\n",
    "\n",
    "# train(args, model, data, rng);\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(params, batch, l2reg):\n",
    "    state, targets = batch  # _rk4\n",
    "    leaves, _ = tree_flatten(params)\n",
    "    l2_norm = sum(jnp.vdot(param, param) for param in leaves)\n",
    "    preds = jax.vmap(partial(baseline_eom, learned_dynamics(params)))(state)\n",
    "    return jnp.sum(jnp.abs(preds - targets)) + l2reg * l2_norm / args.batch_size\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "# def normalize_param_update(param_update):\n",
    "#     new_params = []\n",
    "#     num_weights = args.hidden_dim**2*3\n",
    "#     gradient_norm = sum([jnp.sum(l2**2)\n",
    "#                          for l1 in param_update\n",
    "#                          for l2 in l1\n",
    "#                          if len(l1) != 0])/num_weights\n",
    "# #     gradient_norm = 1 +\n",
    "#     for l1 in param_update:\n",
    "#         if (len(l1)) == 0: new_params.append(()); continue\n",
    "#         new_l1 = []\n",
    "#         for l2 in l1:\n",
    "#             new_l1.append(\n",
    "#                 l2/gradient_norm\n",
    "#             )\n",
    "\n",
    "#         new_params.append(new_l1)\n",
    "\n",
    "#     return new_params\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_derivative(i, opt_state, batch, l2reg):\n",
    "    params = get_params(opt_state)\n",
    "    param_update = jax.grad(lambda *args: loss(*args) / len(batch), 0)(\n",
    "        params, batch, l2reg\n",
    "    )\n",
    "    #     param_update = normalize_param_update(param_update)\n",
    "    params = get_params(opt_state)\n",
    "    return opt_update(i, param_update, opt_state), params\n",
    "\n",
    "\n",
    "best_small_loss = np.inf\n",
    "(nn_forward_fn, init_params) = model\n",
    "iteration = 0\n",
    "total_epochs = 300\n",
    "minibatch_per = 2000\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "lr = 4e-5  # 1e-3\n",
    "\n",
    "final_div_factor = 1e4\n",
    "\n",
    "\n",
    "# OneCycleLR:\n",
    "@jax.jit\n",
    "def OneCycleLR(pct):\n",
    "    # Rush it:\n",
    "    start = 0.2  # 0.2\n",
    "    pct = pct * (1 - start) + start\n",
    "    high, low = lr, lr / final_div_factor\n",
    "\n",
    "    scale = 1.0 - (jnp.cos(2 * jnp.pi * pct) + 1) / 2\n",
    "\n",
    "    return low + (high - low) * scale\n",
    "\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(OneCycleLR)\n",
    "\n",
    "# init_params = custom_init(init_params, seed=0)\n",
    "# init_params =\n",
    "opt_state = opt_init(init_params)\n",
    "# opt_state = opt_init(best_params)\n",
    "bad_iterations = 0\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: add identity before inverse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = get_derivative_dataset(rng)[0][:10], get_derivative_dataset(rng)[1][:10]\n",
    "print(batch_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(get_params(opt_state), batch_data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state, params = update_derivative(0.0, opt_state, batch_data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss = np.inf\n",
    "# best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epoch, total_epochs)):\n",
    "    epoch_loss = 0.0\n",
    "    num_samples = 0\n",
    "    all_batch_data = get_derivative_dataset(rng)\n",
    "    for minibatch in range(minibatch_per):\n",
    "        fraction = (epoch + minibatch / minibatch_per) / total_epochs\n",
    "        batch_data = (\n",
    "            all_batch_data[0][minibatch * batch : (minibatch + 1) * batch],\n",
    "            all_batch_data[1][minibatch * batch : (minibatch + 1) * batch],\n",
    "        )\n",
    "        rng += 10\n",
    "        opt_state, params = update_derivative(fraction, opt_state, batch_data, 1e-6)\n",
    "        cur_loss = loss(params, batch_data, 0.0)\n",
    "        epoch_loss += cur_loss\n",
    "        num_samples += batch\n",
    "    closs = epoch_loss / num_samples\n",
    "    print(\"epoch={} lr={} loss={}\".format(epoch, OneCycleLR(fraction), closs))\n",
    "    if closs < best_loss:\n",
    "        best_loss = closs\n",
    "        best_params = [\n",
    "            [copy(jax.device_get(l2)) for l2 in l1] if len(l1) > 0 else ()\n",
    "            for l1 in params\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at distribution of weights to make a better model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = pkl.load(open(\"best_dblpendulum_baseline_v5_900epoch.pt\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = opt_init(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl.dump(\n",
    "#     best_params,\n",
    "#     open('best_dblpendulum_baseline_v5_900epoch.pt', 'wb')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure the args are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_state = opt_init(loaded['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng + 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seed: [8, 8] looks pretty good! Set args.n_updates=3, and the file params_for_loss_0.29429444670677185_nupdates=1.pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"font\", family=\"serif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_t = 10\n",
    "new_dataset = new_get_dataset(\n",
    "    jax.random.PRNGKey(2),\n",
    "    t_span=[0, max_t],\n",
    "    fps=10,\n",
    "    test_split=1.0,\n",
    "    unlimited_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = new_dataset[\"x\"][0, :]\n",
    "tall = [jax.device_get(t)]\n",
    "p = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[\"x\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tall = jax.device_get(\n",
    "    odeint(\n",
    "        partial(baseline_eom, learned_dynamics(p)),\n",
    "        t,\n",
    "        np.linspace(0, max_t, num=new_dataset[\"x\"].shape[0]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def kinetic_energy(state, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
    "    q, q_dot = jnp.split(state, 2)\n",
    "    (t1, t2), (w1, w2) = q, q_dot\n",
    "\n",
    "    T1 = 0.5 * m1 * (l1 * w1) ** 2\n",
    "    T2 = (\n",
    "        0.5\n",
    "        * m2\n",
    "        * ((l1 * w1) ** 2 + (l2 * w2) ** 2 + 2 * l1 * l2 * w1 * w2 * jnp.cos(t1 - t2))\n",
    "    )\n",
    "    T = T1 + T2\n",
    "    return T\n",
    "\n",
    "\n",
    "@jit\n",
    "def potential_energy(state, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
    "    q, q_dot = jnp.split(state, 2)\n",
    "    (t1, t2), (w1, w2) = q, q_dot\n",
    "\n",
    "    y1 = -l1 * jnp.cos(t1)\n",
    "    y2 = y1 - l2 * jnp.cos(t2)\n",
    "    V = m1 * g * y1 + m2 * g * y2\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tall = np.array(tall)\n",
    "plt.plot(new_dataset[\"x\"][:30, 0])  # [:100, 0])\n",
    "plt.plot(pred_tall[:30, 0])\n",
    "\n",
    "plt.ylabel(r\"$\\theta_1$\")\n",
    "plt.xlabel(\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errors = []\n",
    "for i in tqdm(range(100)):\n",
    "    max_t = 100\n",
    "    new_dataset = new_get_dataset(\n",
    "        jax.random.PRNGKey(i),\n",
    "        t_span=[0, max_t],\n",
    "        fps=10,\n",
    "        test_split=1.0,\n",
    "        unlimited_steps=False,\n",
    "    )\n",
    "    t = new_dataset[\"x\"][0, :]\n",
    "    tall = [jax.device_get(t)]\n",
    "    p = best_params\n",
    "    pred_tall = jax.device_get(\n",
    "        odeint(\n",
    "            partial(baseline_eom, learned_dynamics(p)),\n",
    "            t,\n",
    "            np.linspace(0, max_t, num=new_dataset[\"x\"].shape[0]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    total_true_energy = jax.vmap(kinetic_energy, 0, 0)(new_dataset[\"x\"][:]) + jax.vmap(\n",
    "        potential_energy, 0, 0\n",
    "    )(new_dataset[\"x\"][:])\n",
    "    total_predicted_energy = jax.vmap(kinetic_energy, 0, 0)(pred_tall[:]) + jax.vmap(\n",
    "        potential_energy, 0, 0\n",
    "    )(pred_tall[:])\n",
    "\n",
    "    scale = 29.4\n",
    "\n",
    "    # translation = jnp.min(total_true_energy) + 1\n",
    "    # total_true_energy -= translation\n",
    "    # total_predicted_energy -= translation\n",
    "\n",
    "    cur_error = jnp.abs((total_predicted_energy - total_true_energy)[-1]) / scale\n",
    "    all_errors.append(cur_error)\n",
    "\n",
    "    print(i, \"current error\", jnp.average(all_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can't run the 44th system. Just freezes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_true_energy = jax.vmap(kinetic_energy, 0, 0)(new_dataset[\"x\"][:]) + jax.vmap(\n",
    "    potential_energy, 0, 0\n",
    ")(new_dataset[\"x\"][:])\n",
    "total_predicted_energy = jax.vmap(kinetic_energy, 0, 0)(pred_tall[:]) + jax.vmap(\n",
    "    potential_energy, 0, 0\n",
    ")(pred_tall[:])\n",
    "scale = 29.4\n",
    "\n",
    "# translation = jnp.min(total_true_energy) + 1\n",
    "# total_true_energy -= translation\n",
    "# total_predicted_energy -= translation\n",
    "\n",
    "plt.plot((total_predicted_energy - total_true_energy) / scale)\n",
    "\n",
    "plt.ylabel(\"Absolute Error in Total Energy/Max Potential Energy\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylim(-0.06, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('baseline_dblpend_energy.npy', total_predicted_energy)\n",
    "# np.save('baseline_dblpend_prediction.npy', pred_tall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(int(1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = (\n",
    "    get_derivative_dataset(rng)[0][:100000],\n",
    "    get_derivative_dataset(rng)[1][:100000],\n",
    ")\n",
    "print(batch_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(best_params, batch_data, 0.0) / len(batch_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = np.inf\n",
    "best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _i in range(1000):\n",
    "    print(\"Running\", _i)\n",
    "    print(\"Cur best\", str(best_loss))\n",
    "\n",
    "    init_random_params, nn_forward_fn = extended_mlp(args)\n",
    "\n",
    "    HyperparameterSearch.nn_forward_fn = nn_forward_fn\n",
    "    _, init_params = init_random_params(rng + 1, (-1, 4))\n",
    "    rng += 1\n",
    "    model = (nn_forward_fn, init_params)\n",
    "    opt_init, opt_update, get_params = optimizers.adam(\n",
    "        3e-4\n",
    "    )  ##lambda i: jnp.select([i<10000, i>= 10000], [args.lr, args.lr2]))\n",
    "    opt_state = opt_init(init_params)\n",
    "    loss = make_loss(args)\n",
    "\n",
    "    train(args, model, data, rng)\n",
    "\n",
    "    @jax.jit\n",
    "    def update_derivative(i, opt_state, batch, l2reg):\n",
    "        params = get_params(opt_state)\n",
    "        param_update = jax.grad(loss, 0)(params, batch, l2reg)\n",
    "        leaves, _ = tree_flatten(param_update)\n",
    "        infinities = sum((~jnp.isfinite(param)).sum() for param in leaves)\n",
    "\n",
    "        def true_fun(x):\n",
    "            # No introducing NaNs.\n",
    "            return opt_update(i, param_update, opt_state), params\n",
    "\n",
    "        def false_fun(x):\n",
    "            # No introducing NaNs.\n",
    "            return opt_state, params\n",
    "\n",
    "        return jax.lax.cond(infinities == 0, 0, true_fun, 0, false_fun)\n",
    "\n",
    "    best_small_loss = np.inf\n",
    "    (nn_forward_fn, init_params) = model\n",
    "    data = {k: jax.device_put(v) for k, v in data.items()}\n",
    "    iteration = 0\n",
    "    train_losses, test_losses = [], []\n",
    "    lr = args.lr\n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(init_params)\n",
    "    bad_iterations = 0\n",
    "    offset = 0\n",
    "\n",
    "    while iteration < 20000:\n",
    "        iteration += 1\n",
    "        rand_idx = jax.random.randint(rng, (args.batch_size,), 0, len(data[\"x\"]))\n",
    "        rng += 1\n",
    "\n",
    "        batch = (data[\"x\"][rand_idx], data[\"dx\"][rand_idx])\n",
    "        opt_state, params = update_derivative(\n",
    "            iteration + offset, opt_state, batch, args.l2reg\n",
    "        )\n",
    "        small_loss = loss(params, batch, 0.0)\n",
    "\n",
    "        new_small_loss = False\n",
    "        if small_loss < best_small_loss:\n",
    "            best_small_loss = small_loss\n",
    "            new_small_loss = True\n",
    "\n",
    "        if (\n",
    "            jnp.isnan(small_loss).sum()\n",
    "            or new_small_loss\n",
    "            or (iteration % 500 == 0)\n",
    "            or (iteration < 1000 and iteration % 100 == 0)\n",
    "        ):\n",
    "            params = get_params(opt_state)\n",
    "            train_loss = loss(params, (data[\"x\"], data[\"dx\"]), 0.0) / len(data[\"x\"])\n",
    "            train_losses.append(train_loss)\n",
    "            test_loss = loss(params, (data[\"test_x\"], data[\"test_dx\"]), 0.0) / len(\n",
    "                data[\"test_x\"]\n",
    "            )\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "            if iteration >= 1000 and test_loss > 1.5:\n",
    "                # Only good seeds allowed!\n",
    "                break\n",
    "\n",
    "            if test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                best_params = copy(params)\n",
    "                bad_iterations = 0\n",
    "                offset += iteration\n",
    "                iteration = 0  # Keep going since this one is so good!\n",
    "\n",
    "            if jnp.isnan(test_loss).sum():\n",
    "                break\n",
    "                lr = lr / 2\n",
    "                opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "                opt_state = opt_init(best_params)\n",
    "                bad_iterations = 0\n",
    "\n",
    "            print(\n",
    "                f\"iteration={iteration}, train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\"\n",
    "            )\n",
    "\n",
    "        bad_iterations += 1\n",
    "\n",
    "    if best_loss < np.inf:\n",
    "        pkl.dump(\n",
    "            {\"params\": best_params, \"args\": args},\n",
    "            open(\"params_for_loss_{}_nupdates=1.pkl\".format(best_loss), \"wb\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(best_params, (data[\"test_x\"], data[\"test_dx\"]), 0.0) / len(data[\"test_x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
