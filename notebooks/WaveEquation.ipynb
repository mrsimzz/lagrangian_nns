{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np  # get rid of this eventually\n",
    "from jax import jit\n",
    "from jax.experimental.ode import odeint\n",
    "from functools import partial\n",
    "\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../hyperopt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyperparameterSearch import extended_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectView(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jjit = jax.jit\n",
    "# jjit = lambda _: _\n",
    "ic = lambda *args, **kwargs: None\n",
    "# from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the lagrangian with a parameteric model\n",
    "def learned_dynamics(params):\n",
    "    @jjit\n",
    "    def dynamics(q, q_t):\n",
    "        #     assert q.shape == (2,)\n",
    "        state = jnp.concatenate([q, q_t])\n",
    "        out = nn_forward_fn(params, state)\n",
    "        return jnp.squeeze(out, axis=-1)\n",
    "\n",
    "    return dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_lagrangian_eom(lagrangian, state, t=None):\n",
    "    # state is all q, then all q_t.\n",
    "    vlagrangian = jjit(jax.vmap(lagrangian, (0, 0), 0))\n",
    "\n",
    "    # Sum Lagrangians for all interacting coordinates.\n",
    "    @jjit\n",
    "    def lagrangian_fnc(q, q_t):\n",
    "        # Assume q is [N,]\n",
    "        # Move into [q_i-1, q_i, q_i+1]\n",
    "\n",
    "        q_min = jnp.roll(q, shift=+1)\n",
    "        q_plus = jnp.roll(q, shift=-1)\n",
    "\n",
    "        q_t_min = jnp.roll(q_t, shift=+1)\n",
    "        q_t_plus = jnp.roll(q_t, shift=-1)\n",
    "\n",
    "        all_q = jnp.stack([q_min, q, q_plus], axis=-1)\n",
    "        all_q_t = jnp.stack([q_t_min, q_t, q_t_plus], axis=-1)\n",
    "        ic(all_q.shape, all_q_t.shape)\n",
    "        return jnp.sum(vlagrangian(all_q, all_q_t))\n",
    "\n",
    "    @jjit\n",
    "    def conv_fnc(q, q_t):\n",
    "        # Assume q is [N,]\n",
    "        ic(q.shape, q_t.shape)\n",
    "        q_tt = jnp.linalg.pinv(jax.hessian(lagrangian_fnc, 1)(q, q_t)) @ (\n",
    "            jax.grad(lagrangian_fnc, 0)(q, q_t)\n",
    "            - jax.jacobian(jax.jacobian(lagrangian_fnc, 1), 0)(q, q_t) @ q_t\n",
    "        )\n",
    "        ic(q.shape, q_tt.shape)\n",
    "        return jnp.array([q_t, q_tt])\n",
    "\n",
    "    @jjit\n",
    "    def fnc(state):\n",
    "        q, q_t = jnp.split(state, 2)\n",
    "\n",
    "        out = conv_fnc(q, q_t)\n",
    "        ic(out.shape, \"first\")\n",
    "        out = jnp.concatenate([out[0], out[1]])\n",
    "        ic(out.shape)\n",
    "        return out\n",
    "\n",
    "    return fnc(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's load the best model. To generate more models, see the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded = pkl.load(open('', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ObjectView(\n",
    "    {\n",
    "        \"dataset_size\": 200,\n",
    "        \"fps\": 10,\n",
    "        \"samples\": 100,\n",
    "        \"num_epochs\": 80000,\n",
    "        \"seed\": 0,\n",
    "        \"loss\": \"l1\",\n",
    "        \"act\": \"softplus\",\n",
    "        \"hidden_dim\": 400,\n",
    "        \"input_dim\": 6,  # adjacent points.\n",
    "        \"output_dim\": 1,\n",
    "        \"layers\": 3,\n",
    "        \"n_updates\": 1,\n",
    "        \"lr\": 0.001,\n",
    "        \"lr2\": 2e-05,\n",
    "        \"dt\": 0.1,\n",
    "        \"model\": \"gln\",\n",
    "        \"batch_size\": 512,\n",
    "        \"l2reg\": 5.7e-07,\n",
    "        \"gridsize\": 10,\n",
    "    }\n",
    ")\n",
    "# args = loaded['args']\n",
    "rng = jax.random.PRNGKey(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyperparameterSearch import new_get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 0.1  # (lambda _x: _x[1]-_x[0])(jnp.linspace(0, 1, num=args.gridsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def analytic_fn(state):\n",
    "    q, q_t = jnp.split(state, 2)\n",
    "\n",
    "    q_plus = jnp.roll(q, shift=-1)\n",
    "    q_min = jnp.roll(q, shift=+1)\n",
    "\n",
    "    q_x = (q_plus - q_min) / (2 * dx)\n",
    "    q_xx = (q_plus - 2 * q + q_min) / (2 * dx)\n",
    "\n",
    "    # Wave equation with constraint:\n",
    "    q_tt = q_xx\n",
    "    return jnp.concatenate([q_t, q_tt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = jnp.exp(-((jnp.linspace(0, 1, num=args.gridsize) - 0.5) ** 2) / 2 / 0.05**2)\n",
    "qt0 = jnp.zeros(args.gridsize)\n",
    "state0 = jnp.concatenate([q0, qt0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(q0)\n",
    "plt.plot(analytic_fn(state0)[args.gridsize :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_t = 30\n",
    "all_t = jnp.linspace(0, max_t, num=100)\n",
    "\n",
    "plot_gridsize = args.gridsize\n",
    "\n",
    "q0 = jnp.exp(-((jnp.linspace(0, 1, num=plot_gridsize) - 0.5) ** 2) / 2 / 0.05**2)\n",
    "qt0 = jnp.zeros(plot_gridsize)\n",
    "state0 = jnp.concatenate([q0, qt0])\n",
    "\n",
    "\n",
    "def ofunc(y, t=None):\n",
    "    return analytic_fn(y)\n",
    "\n",
    "\n",
    "state_t = odeint(ofunc, state0, all_t)\n",
    "\n",
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "cam = Camera(fig)\n",
    "\n",
    "for i in range(len(state_t)):\n",
    "    ax.plot((lambda _x: _x - jnp.average(_x))(state_t[i, :plot_gridsize]), c=\"k\")\n",
    "    cam.snap()\n",
    "\n",
    "HTML(cam.animate().to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfnc = jax.jit(jax.vmap(analytic_fn, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 1\n",
    "minibatch_per = 5000\n",
    "\n",
    "\n",
    "@jjit\n",
    "def get_derivative_dataset(rng):\n",
    "    # randomly sample inputs\n",
    "    num = int(args.gridsize / 10)\n",
    "\n",
    "    unsmooth_y0 = jax.random.normal(rng, (batch * minibatch_per, args.gridsize))\n",
    "    unsmooth_yt0 = jax.random.normal(rng + 1, (batch * minibatch_per, args.gridsize))\n",
    "\n",
    "    y0 = jnp.concatenate(\n",
    "        [\n",
    "            sum(\n",
    "                [\n",
    "                    jnp.roll(unsmooth_y0, i, axis=1) * 2 ** (-((num / 2 - i) ** 2) / 10)\n",
    "                    for i in range(num)\n",
    "                ]\n",
    "            ),\n",
    "            sum(\n",
    "                [\n",
    "                    jnp.roll(unsmooth_yt0, i, axis=1)\n",
    "                    * 2 ** (-((num / 2 - i) ** 2) / 10)\n",
    "                    for i in range(num)\n",
    "                ]\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return y0, vfnc(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = None\n",
    "best_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_random_params, nn_forward_fn = extended_mlp(args)\n",
    "import HyperparameterSearch\n",
    "\n",
    "HyperparameterSearch.nn_forward_fn = nn_forward_fn\n",
    "_, init_params = init_random_params(rng + 1, (-1, args.input_dim))\n",
    "rng += 1\n",
    "model = (nn_forward_fn, init_params)\n",
    "opt_init, opt_update, get_params = optimizers.adam(args.lr)\n",
    "opt_state = opt_init([[l2 / 200.0 for l2 in l1] for l1 in init_params])\n",
    "from jax.tree_util import tree_flatten\n",
    "from HyperparameterSearch import make_loss, train\n",
    "from copy import deepcopy as copy\n",
    "# train(args, model, data, rng);\n",
    "\n",
    "\n",
    "@jjit\n",
    "def loss(params, batch, l2reg):\n",
    "    state, targets = batch  # _rk4\n",
    "    #     leaves, _ = tree_flatten(params)\n",
    "    #     l2_norm = sum(jnp.vdot(param, param) for param in leaves)\n",
    "    preds = jax.vmap(partial(raw_lagrangian_eom, learned_dynamics(params)))(state)\n",
    "    ic(preds.shape, targets.shape)\n",
    "    # preds=(1000, 100, 2), targets=(1000, 200)\n",
    "    return jnp.sum(jnp.abs(preds - targets))  # + l2reg*l2_norm/args.batch_size\n",
    "\n",
    "\n",
    "@jjit\n",
    "def update_derivative(i, opt_state, batch, l2reg):\n",
    "    params = get_params(opt_state)\n",
    "    param_update = jax.grad(lambda *args: loss(*args) / len(batch), 0)(\n",
    "        params, batch, l2reg\n",
    "    )\n",
    "    #     param_update = normalize_param_update(param_update)\n",
    "    params = get_params(opt_state)\n",
    "    return opt_update(i, param_update, opt_state), params\n",
    "\n",
    "\n",
    "best_small_loss = np.inf\n",
    "(nn_forward_fn, init_params) = model\n",
    "iteration = 0\n",
    "total_epochs = 30\n",
    "train_losses, test_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_forward_fn(init_params, jnp.zeros((10, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3  # 1e-3\n",
    "\n",
    "final_div_factor = 1e4\n",
    "\n",
    "\n",
    "# OneCycleLR:\n",
    "@jjit\n",
    "def OneCycleLR(pct):\n",
    "    # Rush it:\n",
    "    start = 0.3  # 0.2\n",
    "    pct = pct * (1 - start) + start\n",
    "    high, low = lr, lr / final_div_factor\n",
    "\n",
    "    scale = 1.0 - (jnp.cos(2 * jnp.pi * pct) + 1) / 2\n",
    "\n",
    "    return low + (high - low) * scale\n",
    "\n",
    "\n",
    "from lnn import custom_init\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(OneCycleLR)\n",
    "\n",
    "init_params = custom_init(init_params, seed=1)\n",
    "\n",
    "opt_state = opt_init(init_params)\n",
    "# opt_state = opt_init(best_params)\n",
    "bad_iterations = 0\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: add identity before inverse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = (\n",
    "    get_derivative_dataset(rng)[0][:1000],\n",
    "    get_derivative_dataset(rng)[1][:1000],\n",
    ")\n",
    "print(batch_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(batch_data[0][0])\n",
    "plt.figure()\n",
    "plt.plot(batch_data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(init_params, [_x[:1] for _x in batch_data], 0.0) / len(batch_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_derivative(0.0, opt_state, [_x[:1] for _x in batch_data], 0.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_loss = np.inf\n",
    "# best_params = None\n",
    "total_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    for epoch in tqdm(range(epoch, total_epochs)):\n",
    "        epoch_loss = 0.0\n",
    "        num_samples = 0\n",
    "        all_batch_data = get_derivative_dataset(rng)\n",
    "        for minibatch in range(minibatch_per):\n",
    "            fraction = (epoch + minibatch / minibatch_per) / total_epochs\n",
    "            batch_data = (\n",
    "                all_batch_data[0][minibatch * batch : (minibatch + 1) * batch],\n",
    "                all_batch_data[1][minibatch * batch : (minibatch + 1) * batch],\n",
    "            )\n",
    "            if batch_data[0].shape[0] == 0:\n",
    "                break\n",
    "            rng += 10\n",
    "            opt_state, params = update_derivative(fraction, opt_state, batch_data, 1e-6)\n",
    "            cur_loss = loss(params, batch_data, 0.0)\n",
    "            epoch_loss += cur_loss\n",
    "            num_samples += batch\n",
    "        closs = epoch_loss / num_samples\n",
    "        print(\"epoch={} lr={} loss={}\".format(epoch, OneCycleLR(fraction), closs))\n",
    "        if closs < best_loss:\n",
    "            best_loss = closs\n",
    "            best_params = [\n",
    "                [copy(jax.device_get(l2)) for l2 in l1] if len(l1) > 0 else ()\n",
    "                for l1 in params\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = opt_init(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl.dump(\n",
    "#     best_params,\n",
    "#     open('best_wave_equation_full_v1_{}.pt'.format(best_loss), 'wb')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = pkl.load(open('best_wave_equation_v2_0.4340578317642212.pt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_state = opt_init(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure the args are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_state = opt_init(loaded['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng + 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_t = 300\n",
    "all_t = jnp.linspace(0, max_t, num=max_t * 5)\n",
    "\n",
    "plot_gridsize = 100  # args.gridsize\n",
    "\n",
    "q0 = jnp.exp(-((jnp.linspace(0, 1, num=plot_gridsize) - 0.5) ** 2) / 2 / (0.05) ** 2)\n",
    "qt0 = jnp.zeros(plot_gridsize)\n",
    "state0 = jnp.concatenate([q0, qt0])\n",
    "\n",
    "\n",
    "def ofunc(y, t=None):\n",
    "    return analytic_fn(y)\n",
    "\n",
    "\n",
    "state_t = odeint(ofunc, state0, all_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = state_t\n",
    "tall = jax.device_get(t)\n",
    "p = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tall = jax.device_get(\n",
    "    odeint(partial(raw_lagrangian_eom, learned_dynamics(p)), state0, all_t)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tall.shape, tall.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(state):\n",
    "    # state is [q, qt]\n",
    "    q, q_t = jnp.split(state, 2)\n",
    "    # Invariance:\n",
    "    q_t -= jnp.average(q_t)\n",
    "    q_plus = jnp.roll(q, shift=-1)\n",
    "    q_min = jnp.roll(q, shift=+1)\n",
    "    q_x = (q_plus - q_min) / (2 * 0.02040816)\n",
    "\n",
    "    E = jnp.sum(0.5 * q_t**2 * 455.88 + 0.5 * q_x**2)\n",
    "    return E\n",
    "\n",
    "\n",
    "venergy = jax.vmap(energy, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_en = venergy(tall)\n",
    "pred_en = venergy(pred_tall)\n",
    "\n",
    "plt.plot(true_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"font\", family=\"serif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celluloid import Camera\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig, allax = plt.subplots(2, 1, figsize=(4 * 3, 3 * 2))\n",
    "# allax = allax.T\n",
    "mmult = 1.5\n",
    "\n",
    "# Swap zone:\n",
    "cam = Camera(fig)\n",
    "# for j, i in enumerate((np.r_[0, 100, 499]*mmult).astype(int)):\n",
    "for i in tqdm(\n",
    "    range(0, int(499 * mmult), 2)\n",
    "):  # len(pred_tall), 1))):#tqdm(range(0, len(pred_tall), 1)):\n",
    "    #     ax = allax[j]\n",
    "\n",
    "    ax = allax\n",
    "    cax = ax[0]\n",
    "    cax.plot(\n",
    "        np.arange(plot_gridsize),\n",
    "        (lambda _: _ - np.average(_))(tall[i, :plot_gridsize]),\n",
    "        color=\"b\",\n",
    "        linewidth=2,\n",
    "        label=\"Truth\",\n",
    "    )\n",
    "    cax.plot(\n",
    "        np.arange(plot_gridsize),\n",
    "        (lambda _: _ - np.average(_))(pred_tall[i, :plot_gridsize]),\n",
    "        linestyle=\"--\",\n",
    "        color=\"orange\",\n",
    "        linewidth=2,\n",
    "        label=\"Prediction\",\n",
    "    )\n",
    "    cax.set_xlabel(\"Grid Index\")\n",
    "    cax.set_ylabel(\"Relative Amplitude\")\n",
    "    cax.set_ylim(-0.5, 1.0)\n",
    "\n",
    "    cax = ax[1]\n",
    "    (t1,) = cax.plot(\n",
    "        np.arange(i + 1), true_en[: i + 1], color=\"b\", linewidth=2, label=\"Truth\"\n",
    "    )\n",
    "    (t2,) = cax.plot(\n",
    "        np.arange(i + 1),\n",
    "        pred_en[: i + 1],\n",
    "        linestyle=\"--\",\n",
    "        color=\"orange\",\n",
    "        linewidth=2,\n",
    "        label=\"Prediction\",\n",
    "    )\n",
    "    cax.scatter(i, energy(pred_tall[i]), color=\"k\")\n",
    "    cax.set_xlabel(\"Time\")\n",
    "    cax.set_ylabel(\"Total Energy\")\n",
    "    cax.set_ylim(0, 250)\n",
    "    cax.set_xlim(-20, 520 * mmult)\n",
    "    cax.legend([t1, t2], [\"Truth\", \"Prediction\"], loc=\"lower left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    cam.snap()\n",
    "\n",
    "ani = cam.animate()\n",
    "HTML(ani.to_jshtml())\n",
    "\n",
    "# plt.savefig('wave.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save(\"wave_equation.mp4\", fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save(\"wave_equation.gif\", writer=\"imagemagick\", fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def kinetic_energy(state, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
    "    q, q_dot = jnp.split(state, 2)\n",
    "    (t1, t2), (w1, w2) = q, q_dot\n",
    "\n",
    "    T1 = 0.5 * m1 * (l1 * w1) ** 2\n",
    "    T2 = (\n",
    "        0.5\n",
    "        * m2\n",
    "        * ((l1 * w1) ** 2 + (l2 * w2) ** 2 + 2 * l1 * l2 * w1 * w2 * jnp.cos(t1 - t2))\n",
    "    )\n",
    "    T = T1 + T2\n",
    "    return T\n",
    "\n",
    "\n",
    "@jit\n",
    "def potential_energy(state, m1=1, m2=1, l1=1, l2=1, g=9.8):\n",
    "    q, q_dot = jnp.split(state, 2)\n",
    "    (t1, t2), (w1, w2) = q, q_dot\n",
    "\n",
    "    y1 = -l1 * jnp.cos(t1)\n",
    "    y2 = y1 - l2 * jnp.cos(t2)\n",
    "    V = m1 * g * y1 + m2 * g * y2\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"font\", family=\"serif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare energy for a variety of initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errors = []\n",
    "for i in tqdm(range(40)):\n",
    "    max_t = 100\n",
    "    new_dataset = new_get_dataset(\n",
    "        jax.random.PRNGKey(i),\n",
    "        t_span=[0, max_t],\n",
    "        fps=10,\n",
    "        test_split=1.0,\n",
    "        unlimited_steps=False,\n",
    "    )\n",
    "    t = new_dataset[\"x\"][0, :]\n",
    "    tall = [jax.device_get(t)]\n",
    "    p = best_params\n",
    "    pred_tall = jax.device_get(\n",
    "        odeint(\n",
    "            partial(raw_lagrangian_eom, learned_dynamics(p)),\n",
    "            t,\n",
    "            np.linspace(0, max_t, num=new_dataset[\"x\"].shape[0]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    total_true_energy = jax.vmap(kinetic_energy, 0, 0)(new_dataset[\"x\"][:]) + jax.vmap(\n",
    "        potential_energy, 0, 0\n",
    "    )(new_dataset[\"x\"][:])\n",
    "    total_predicted_energy = jax.vmap(kinetic_energy, 0, 0)(pred_tall[:]) + jax.vmap(\n",
    "        potential_energy, 0, 0\n",
    "    )(pred_tall[:])\n",
    "\n",
    "    scale = 29.4\n",
    "\n",
    "    # translation = jnp.min(total_true_energy) + 1\n",
    "    # total_true_energy -= translation\n",
    "    # total_predicted_energy -= translation\n",
    "\n",
    "    cur_error = jnp.abs((total_predicted_energy - total_true_energy)[-1]) / scale\n",
    "    all_errors.append(cur_error)\n",
    "\n",
    "    print(i, \"current error\", jnp.average(all_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots made down here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_predicted_energy_b = np.load(\"baseline_dblpend_energy.npy\")\n",
    "pred_tall_b = np.load(\"baseline_dblpend_prediction.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tall = np.array(tall)\n",
    "plt.plot(new_dataset[\"x\"][:500, 0])\n",
    "plt.plot(pred_tall[:500, 0])  # [:100, 0])\n",
    "plt.ylabel(r\"$\\theta_1$\")\n",
    "plt.xlabel(\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.max(jax.vmap(kinetic_energy, 0, 0)(new_dataset[\"x\"][:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.max(jnp.abs(jax.vmap(potential_energy, 0, 0)(new_dataset[\"x\"][:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the scale of the system as the max potential energy of the double\n",
    "pendulum:\n",
    "\n",
    "$9.8\\times1\\times1 + 9.8\\times1\\times2=29.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_true_energy = jax.vmap(kinetic_energy, 0, 0)(new_dataset[\"x\"][:]) + jax.vmap(\n",
    "    potential_energy, 0, 0\n",
    ")(new_dataset[\"x\"][:])\n",
    "total_predicted_energy = jax.vmap(kinetic_energy, 0, 0)(pred_tall[:]) + jax.vmap(\n",
    "    potential_energy, 0, 0\n",
    ")(pred_tall[:])\n",
    "\n",
    "scale = 29.4\n",
    "\n",
    "# translation = jnp.min(total_true_energy) + 1\n",
    "# total_true_energy -= translation\n",
    "# total_predicted_energy -= translation\n",
    "\n",
    "plt.plot(jnp.abs(total_predicted_energy - total_true_energy) / scale)\n",
    "\n",
    "plt.ylabel(\"Absolute Error in Total Energy/Max Potential Energy\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylim(-0.06, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(int(1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = (\n",
    "    get_derivative_dataset(rng)[0][:100000],\n",
    "    get_derivative_dataset(rng)[1][:100000],\n",
    ")\n",
    "print(batch_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(best_params, batch_data, 0.0) / len(batch_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('lnn_dblpend_energy.npy', total_predicted_energy)\n",
    "# np.save('lnn_dblpend_prediction.npy', pred_tall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tall = np.array(tall)\n",
    "fig, ax = plt.subplots(2, 2, sharey=True)\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    if i == 1:\n",
    "        start = 1400\n",
    "        end = 1500\n",
    "    if i == 0:\n",
    "        start = 0\n",
    "        end = 100\n",
    "\n",
    "    dom = np.linspace(start / 10, end / 10, num=end - start)\n",
    "    ax[0, i].plot(dom, pred_tall[start:end, 0], label=\"LNN\")  # [:100, 0])\n",
    "    ax[0, i].plot(dom, pred_tall_b[start:end, 0], label=\"Baseline\")  # [:100, 0])\n",
    "    ax[0, i].plot(dom, new_dataset[\"x\"][start:end, 0], label=\"Truth\")\n",
    "    # ax[0].set_xlabel('Time')\n",
    "    ax[1, i].plot(\n",
    "        dom, -new_dataset[\"x\"][start:end, 0] + pred_tall[start:end, 0], label=\"LNN\"\n",
    "    )  # [:100, 0])\n",
    "    ax[1, i].plot(\n",
    "        dom,\n",
    "        -new_dataset[\"x\"][start:end, 0] + pred_tall_b[start:end, 0],\n",
    "        label=\"Baseline\",\n",
    "    )  # [:100, 0])\n",
    "    if i == 0:\n",
    "        ax[0, i].set_ylabel(r\"$\\theta_1$\")\n",
    "        ax[1, i].set_ylabel(r\"Error in $\\theta_1$\")\n",
    "\n",
    "    ax[1, i].set_xlabel(\"Time\")\n",
    "    if i == 0:\n",
    "        ax[0, i].legend()\n",
    "        ax[1, i].legend()\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i, 0].spines[\"right\"].set_visible(False)\n",
    "    ax[i, 1].spines[\"left\"].set_visible(False)\n",
    "    #     ax[i, 0].yaxis.tick_left()\n",
    "    #     ax[i, 0].tick_params(labelright='off')\n",
    "    ax[i, 1].yaxis.tick_right()\n",
    "\n",
    "for i in range(2):\n",
    "    d = 0.015  # how big to make the diagonal lines in axes coordinates\n",
    "    # arguments to pass plot, just so we don't keep repeating them\n",
    "    kwargs = dict(transform=ax[i, 0].transAxes, color=\"k\", clip_on=False)\n",
    "    ax[i, 0].plot((1 - d, 1 + d), (-d, +d), **kwargs)\n",
    "    ax[i, 0].plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)\n",
    "    kwargs.update(transform=ax[i, 1].transAxes)  # switch to the bottom axes\n",
    "    ax[i, 1].plot((-d, +d), (1 - d, 1 + d), **kwargs)\n",
    "    ax[i, 1].plot((-d, +d), (-d, +d), **kwargs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"discrepancy_plot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 9999\n",
    "dom = np.linspace(start / 10, end / 10, num=end - start)\n",
    "\n",
    "scale = 29.4\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "# translation = jnp.min(total_true_energy) + 1\n",
    "# total_true_energy -= translation\n",
    "# total_predicted_energy -= translation\n",
    "\n",
    "ax[0].plot(dom, (total_predicted_energy), label=\"LNN\")\n",
    "ax[0].plot(dom, (total_predicted_energy_b), label=\"Baseline\")\n",
    "ax[0].plot(dom, (total_true_energy), label=\"Truth\")\n",
    "ax[0].set_ylabel(\"Total Energy\")\n",
    "ax[0].set_xlabel(\"Time\")\n",
    "ax[0].set_ylim(-15, 0)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(dom, (total_predicted_energy - total_true_energy) / scale, label=\"LNN\")\n",
    "ax[1].plot(\n",
    "    dom, (total_predicted_energy_b - total_true_energy) / scale, label=\"Baseline\"\n",
    ")\n",
    "ax[1].set_ylabel(\"Error in Total Energy\\n/Max Potential Energy\")\n",
    "ax[1].set_xlabel(\"Time\")\n",
    "ax[1].set_ylim(-0.06, 0.01)\n",
    "ax[1].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"energy_discrepancy_plot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = np.inf\n",
    "best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _i in range(1000):\n",
    "    print(\"Running\", _i)\n",
    "    print(\"Cur best\", str(best_loss))\n",
    "\n",
    "    init_random_params, nn_forward_fn = extended_mlp(args)\n",
    "    import HyperparameterSearch\n",
    "\n",
    "    HyperparameterSearch.nn_forward_fn = nn_forward_fn\n",
    "    _, init_params = init_random_params(rng + 1, (-1, 4))\n",
    "    rng += 1\n",
    "    model = (nn_forward_fn, init_params)\n",
    "    opt_init, opt_update, get_params = optimizers.adam(\n",
    "        3e-4\n",
    "    )  ##lambda i: jnp.select([i<10000, i>= 10000], [args.lr, args.lr2]))\n",
    "    opt_state = opt_init(init_params)\n",
    "    loss = make_loss(args)\n",
    "    from copy import deepcopy as copy\n",
    "\n",
    "    train(args, model, data, rng)\n",
    "\n",
    "    @jax.jit\n",
    "    def update_derivative(i, opt_state, batch, l2reg):\n",
    "        params = get_params(opt_state)\n",
    "        param_update = jax.grad(loss, 0)(params, batch, l2reg)\n",
    "        leaves, _ = tree_flatten(param_update)\n",
    "        infinities = sum((~jnp.isfinite(param)).sum() for param in leaves)\n",
    "\n",
    "        def true_fun(x):\n",
    "            # No introducing NaNs.\n",
    "            return opt_update(i, param_update, opt_state), params\n",
    "\n",
    "        def false_fun(x):\n",
    "            # No introducing NaNs.\n",
    "            return opt_state, params\n",
    "\n",
    "        return jax.lax.cond(infinities == 0, 0, true_fun, 0, false_fun)\n",
    "\n",
    "    best_small_loss = np.inf\n",
    "    (nn_forward_fn, init_params) = model\n",
    "    data = {k: jax.device_put(v) for k, v in data.items()}\n",
    "    iteration = 0\n",
    "    train_losses, test_losses = [], []\n",
    "    lr = args.lr\n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(init_params)\n",
    "    bad_iterations = 0\n",
    "    offset = 0\n",
    "\n",
    "    while iteration < 20000:\n",
    "        iteration += 1\n",
    "        rand_idx = jax.random.randint(rng, (args.batch_size,), 0, len(data[\"x\"]))\n",
    "        rng += 1\n",
    "\n",
    "        batch = (data[\"x\"][rand_idx], data[\"dx\"][rand_idx])\n",
    "        opt_state, params = update_derivative(\n",
    "            iteration + offset, opt_state, batch, args.l2reg\n",
    "        )\n",
    "        small_loss = loss(params, batch, 0.0)\n",
    "\n",
    "        new_small_loss = False\n",
    "        if small_loss < best_small_loss:\n",
    "            best_small_loss = small_loss\n",
    "            new_small_loss = True\n",
    "\n",
    "        if (\n",
    "            jnp.isnan(small_loss).sum()\n",
    "            or new_small_loss\n",
    "            or (iteration % 500 == 0)\n",
    "            or (iteration < 1000 and iteration % 100 == 0)\n",
    "        ):\n",
    "            params = get_params(opt_state)\n",
    "            train_loss = loss(params, (data[\"x\"], data[\"dx\"]), 0.0) / len(data[\"x\"])\n",
    "            train_losses.append(train_loss)\n",
    "            test_loss = loss(params, (data[\"test_x\"], data[\"test_dx\"]), 0.0) / len(\n",
    "                data[\"test_x\"]\n",
    "            )\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "            if iteration >= 1000 and test_loss > 1.5:\n",
    "                # Only good seeds allowed!\n",
    "                break\n",
    "\n",
    "            if test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                best_params = copy(params)\n",
    "                bad_iterations = 0\n",
    "                offset += iteration\n",
    "                iteration = 0  # Keep going since this one is so good!\n",
    "\n",
    "            if jnp.isnan(test_loss).sum():\n",
    "                break\n",
    "                lr = lr / 2\n",
    "                opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "                opt_state = opt_init(best_params)\n",
    "                bad_iterations = 0\n",
    "\n",
    "            print(\n",
    "                f\"iteration={iteration}, train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\"\n",
    "            )\n",
    "\n",
    "        bad_iterations += 1\n",
    "\n",
    "    if best_loss < np.inf:\n",
    "        pkl.dump(\n",
    "            {\"params\": best_params, \"args\": args},\n",
    "            open(\"params_for_loss_{}_nupdates=1.pkl\".format(best_loss), \"wb\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(lnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(best_params, (data[\"test_x\"], data[\"test_dx\"]), 0.0) / len(data[\"test_x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main2",
   "language": "python",
   "name": "main2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
