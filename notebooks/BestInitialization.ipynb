{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from jax import grad, vmap\n",
    "from jax.example_libraries import optimizers\n",
    "\n",
    "from lnn.experiment_dblpend.lnn import raw_lagrangian_eom\n",
    "from lnn.experiment_dblpend.data import get_trajectory_analytic\n",
    "from lnn.experiment_dblpend.physics import analytical_fn\n",
    "from lnn.hyperopt import HyperparameterSearch\n",
    "from lnn.hyperopt.HyperparameterSearch import learned_dynamics\n",
    "from lnn.hyperopt.HyperparameterSearch import extended_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectView(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfnc = jax.jit(jax.vmap(analytical_fn))\n",
    "vget = partial(jax.jit, backend=\"cpu\")(\n",
    "    jax.vmap(\n",
    "        partial(\n",
    "            get_trajectory_analytic,\n",
    "        ),\n",
    "        (0, None),\n",
    "        0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are our model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ObjectView(\n",
    "    {\n",
    "        \"dataset_size\": 200,\n",
    "        \"fps\": 10,\n",
    "        \"samples\": 100,\n",
    "        \"num_epochs\": 80000,\n",
    "        \"seed\": 0,\n",
    "        \"loss\": \"l1\",\n",
    "        \"act\": \"softplus\",\n",
    "        \"hidden_dim\": 30,\n",
    "        \"output_dim\": 1,\n",
    "        \"layers\": 3,\n",
    "        \"n_updates\": 1,\n",
    "        \"lr\": 0.001,\n",
    "        \"lr2\": 2e-05,\n",
    "        \"dt\": 0.1,\n",
    "        \"model\": \"gln\",\n",
    "        \"batch_size\": 68,\n",
    "        \"l2reg\": 5.7e-07,\n",
    "    }\n",
    ")\n",
    "# args = loaded['args']\n",
    "rng = jax.random.PRNGKey(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfnc = jax.jit(jax.vmap(analytical_fn, 0, 0))\n",
    "vget = partial(jax.jit, backend=\"cpu\")(\n",
    "    jax.vmap(\n",
    "        partial(\n",
    "            get_trajectory_analytic,\n",
    "        ),\n",
    "        (0, None),\n",
    "        0,\n",
    "    )\n",
    ")\n",
    "\n",
    "batch = 60\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def get_derivative_dataset(rng):\n",
    "    # randomly sample inputs\n",
    "\n",
    "    y0 = jnp.concatenate(\n",
    "        [\n",
    "            jax.random.uniform(rng, (batch, 2)) * 2.0 * np.pi,\n",
    "            (jax.random.uniform(rng + 1, (batch, 2)) - 0.5) * 10 * 2,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return y0, vfnc(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = None\n",
    "best_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_random_params, nn_forward_fn = extended_mlp(args)\n",
    "\n",
    "HyperparameterSearch.nn_forward_fn = nn_forward_fn\n",
    "_, init_params = init_random_params(rng + 1, (-1, 4))\n",
    "rng += 1\n",
    "model = (nn_forward_fn, init_params)\n",
    "opt_init, opt_update, get_params = optimizers.adam(args.lr)\n",
    "opt_state = opt_init(init_params)\n",
    "# train(args, model, data, rng);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current std:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperparameterSearch.nn_forward_fn = nn_forward_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's score the qdotdot output over normally distributed input for 256 batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = True\n",
    "n = 256\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def custom_init(stds, rng2):\n",
    "    new_params = []\n",
    "    i = 0\n",
    "    for l1 in init_params:\n",
    "        if (len(l1)) == 0:\n",
    "            new_params.append(())\n",
    "            continue\n",
    "        new_l1 = []\n",
    "        for l2 in l1:\n",
    "            if len(l2.shape) == 1:\n",
    "                new_l1.append(jnp.zeros_like(l2))\n",
    "            else:\n",
    "                if normal:\n",
    "                    new_l1.append(jax.random.normal(rng2, l2.shape) * stds[i])\n",
    "                #                     n1 = l2.shape[0]\n",
    "                #                     n2 = l2.shape[1]\n",
    "                #                     power = stds[0]\n",
    "                #                     base_scale = stds[1]\n",
    "                #                     s = base_scale/(n1+n2)**power\n",
    "                #                     new_l1.append(jax.random.normal(rng2, l2.shape)*s)\n",
    "                else:\n",
    "                    new_l1.append(\n",
    "                        jax.random.uniform(rng2, l2.shape, minval=-0.5, maxval=0.5)\n",
    "                        * stds[i]\n",
    "                    )\n",
    "                rng2 += 1\n",
    "                i += 1\n",
    "\n",
    "        new_params.append(new_l1)\n",
    "\n",
    "    return new_params\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def j_score_init(stds, rng2):\n",
    "    new_params = custom_init(stds, rng2)\n",
    "\n",
    "    rand_input = jax.random.normal(rng2, [n, 4])\n",
    "    rng2 += 1\n",
    "\n",
    "    outputs = jax.vmap(partial(raw_lagrangian_eom, learned_dynamics(new_params)))(\n",
    "        rand_input\n",
    "    )[:, 2:]\n",
    "\n",
    "    # KL-divergence to mu=0, std=1:\n",
    "    mu = jnp.average(outputs, axis=0)\n",
    "    std = jnp.std(outputs, axis=0)\n",
    "\n",
    "    KL = jnp.sum((mu**2 + std**2 - 1) / 2.0 - jnp.log(std))\n",
    "\n",
    "    def total_output(p):\n",
    "        return vmap(partial(raw_lagrangian_eom, learned_dynamics(p)))(rand_input).sum()\n",
    "\n",
    "    d_params = grad(total_output)(new_params)\n",
    "\n",
    "    for l1 in d_params:\n",
    "        if (len(l1)) == 0:\n",
    "            continue\n",
    "        for l2 in l1:\n",
    "            if len(l2.shape) == 1:\n",
    "                continue\n",
    "\n",
    "            mu = jnp.average(l2)\n",
    "            std = jnp.std(l2)\n",
    "            KL += (mu**2 + std**2 - 1) / 2.0 - jnp.log(std)\n",
    "\n",
    "    # HACK\n",
    "    #     KL += jnp.sum(stds**2)\n",
    "    return jnp.log10(KL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_std = jnp.array([0.01] * (args.layers + 1))\n",
    "\n",
    "rng2 = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_score_init(cur_std, rng2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "\n",
    "vv = jax.jit(vmap(j_score_init, (None, 0), 0))\n",
    "\n",
    "rng2 = jax.random.PRNGKey(0)\n",
    "\n",
    "\n",
    "def score_init(stds):\n",
    "    global rng2\n",
    "    stds = jnp.array(stds)\n",
    "    stds = jnp.exp(stds)\n",
    "    q75, q50, q25 = np.percentile(\n",
    "        vv(stds, jax.random.split(rng2, num=10)), [75, 50, 25]\n",
    "    )\n",
    "    rng2 += 30\n",
    "\n",
    "    return q50, q75 - q25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_init(cur_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bayes_opt import BayesianOptimization\n",
    "\n",
    "# # Bounded region of parameter space\n",
    "pbounds = {\"s%d\" % (i,): (-15, 15) for i in range(len(cur_std))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb(**kwargs):\n",
    "    out, std = score_init(\n",
    "        [kwargs[q] for q in [\"s%d\" % (i,) for i in range(len(cur_std))]]\n",
    "    )\n",
    "    #     if out is None or not out > -30:\n",
    "    #         return -30.0\n",
    "    return -out, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the best distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's redo that with Bayes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "\n",
    "\n",
    "def run_trial(args):\n",
    "    loss, std = bb(**args)\n",
    "    if loss == np.nan:\n",
    "        return {\n",
    "            \"status\": \"fail\",  # or 'fail' if nan loss\n",
    "            \"loss\": np.inf,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"status\": \"ok\",  # or 'fail' if nan loss\n",
    "        \"loss\": -loss,\n",
    "        \"loss_variance\": std,\n",
    "    }\n",
    "\n",
    "\n",
    "# TODO: Declare your hyperparameter priors here:\n",
    "space = {\n",
    "    **{\"s%d\" % (i,): hp.normal(\"s%d\" % (i,), -2, 5) for i in range(len(cur_std) - 1)},\n",
    "    **{\"s%d\" % (len(cur_std) - 1,): hp.normal(\"s%d\" % (len(cur_std) - 1,), 3, 8)},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = fmin(\n",
    "    run_trial, space=space, algo=tpe.suggest, max_evals=5000, trials=trials, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k(t):\n",
    "    if \"loss\" not in t[\"result\"]:\n",
    "        return np.inf\n",
    "    return t[\"result\"][\"loss\"]\n",
    "\n",
    "\n",
    "sorted_trials = sorted(trials.trials, key=k)\n",
    "len(trials.trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.array(\n",
    "    [\n",
    "        [s[\"misc\"][\"vals\"][\"s%d\" % (i,)][0] for i in range(len(cur_std))]\n",
    "        for s in sorted_trials[:100]\n",
    "    ]\n",
    ")\n",
    "q[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 layers, 1000 hidden: {(4, 1000), (1000, 1000), (1000, 1000), (1000, 1)}\n",
    "\n",
    "## median top 10/2000: array([-1.47842217, -4.37217279, -3.37083752, 11.13480387])\n",
    "\n",
    "(unconverged)\n",
    "\n",
    "## 4 layers, 100 hidden:  {(4, 100), (100, 100), (100, 100), (100, 1)}\n",
    "\n",
    "## median top 30/5000: array([-1.70680816, -2.40340615, -2.17201716, 10.55268474])\n",
    "\n",
    "(unconverged)\n",
    "\n",
    "## 3 layers, 100 hidden:\n",
    "\n",
    "## median top 100/7000: array([-1.69875614, -2.74589338,  3.75818009])\n",
    "\n",
    "(unverged converged)\n",
    "\n",
    "## 3 layers, 30 hidden:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Eureqa to get the scalings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data = np.array(\n",
    "    [\n",
    "        [t[\"misc\"][\"vals\"][\"s%d\" % (i,)][0] for i in range(len(cur_std))]\n",
    "        + [t[\"result\"][\"loss\"]]\n",
    "        for t in trials.trials\n",
    "        if \"loss\" in t[\"result\"] and np.isfinite(t[\"result\"][\"loss\"])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('sdata.npy', simple_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianProcessRegressor(alpha=3, n_restarts_optimizer=20, normalize_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data[:, -1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.fit(simple_data[:, :-1], simple_data[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.layers + 1, args.hidden_dim, q[gp.predict(q).argmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New runs with noise added:\n",
    "\n",
    "## layers, hidden, log(std): (4, 30, array([-2.12770715, -1.99764457, -1.29472256,  6.1514019 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicted with GP for 3 layers, 100 hidden, {(4, 100), (100, 100), (100, 1)}\n",
    "\n",
    "array([-1.95669793, -2.39555616,  1.92755129])\n",
    "\n",
    "## predicted with GP for 3 layers, 50 hidden, {(4, 50), (50, 50), (50, 1)}\n",
    "\n",
    "array([-1.77223004, -3.2154843 , 10.38542243])\n",
    "\n",
    "\n",
    "## predicted with GP for 3 layers, 30 hidden:\n",
    "\n",
    "array([-1.47298021, -4.10931435,  2.60899782])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(simple_data[:, 0], simple_data[:, 1], c=simple_data[:, -1])\n",
    "# plt.ylim(-5, 2)\n",
    "# plt.xlim(-5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num= 50\n",
    "# x = np.linspace(-12, 2, num=num)\n",
    "# y = np.linspace(-12, 2, num=num)\n",
    "# X,Y = np.meshgrid(x, y) # grid of point\n",
    "# Z = gp.predict(np.stack((X.ravel(), Y.ravel()), axis=1)).reshape(*[num]*2) # evaluation of the function on the grid\n",
    "\n",
    "# im = plt.imshow(np.log10(Z), cmap='viridis', extent=[-12, 2, -12, 2], origin='lower')\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=bb,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=4 ** len(cur_std),\n",
    "    n_iter=300 + 4 ** len(cur_std),\n",
    "    alpha=1e-2,\n",
    "    normalize_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l1 in init_params:\n",
    "    if (len(l1)) == 0:\n",
    "        continue\n",
    "    for l2 in l1:\n",
    "        print(l2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data = np.array(\n",
    "    [\n",
    "        [t[\"params\"][\"s%d\" % (i,)] for i in range(len(cur_std))] + [t[\"target\"]]\n",
    "        for t in optimizer.res\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    \"hidden={}_layers={}_results.npy\".format(args.hidden_dim, args.layers), simple_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianProcessRegressor(alpha=1e-2, n_restarts_optimizer=20, normalize_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_data[:, -1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.fit(simple_data[:, :-1], simple_data[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data[:, :-1][gp.predict(simple_data[:, :-1]).argmin()]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
