{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np # get rid of this eventually\n",
    "import argparse\n",
    "from jax import jit\n",
    "from jax.experimental.ode import odeint\n",
    "from functools import partial # reduces arguments to function by making some subset implicit\n",
    "\n",
    "from jax.experimental import stax\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import os, sys, time\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../experiment_dblpend/')\n",
    "\n",
    "from lnn import raw_lagrangian_eom\n",
    "from data import get_dataset\n",
    "from models import mlp as make_mlp\n",
    "from utils import wrap_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../hyperopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyperparameterSearch import learned_dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyperparameterSearch import extended_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectView(object):\n",
    "    def __init__(self, d): self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_trajectory_analytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from physics import analytical_fn\n",
    "\n",
    "vfnc = jax.jit(jax.vmap(analytical_fn))\n",
    "vget = partial(jax.jit, backend='cpu')(jax.vmap(partial(get_trajectory_analytic, ), (0, None), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are our model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ObjectView({'dataset_size': 200,\n",
    " 'fps': 10,\n",
    " 'samples': 100,\n",
    " 'num_epochs': 80000,\n",
    " 'seed': 0,\n",
    " 'loss': 'l1',\n",
    " 'act': 'softplus',\n",
    " 'hidden_dim': 30,\n",
    " 'output_dim': 1,\n",
    " 'layers': 3,\n",
    " 'n_updates': 1,\n",
    " 'lr': 0.001,\n",
    " 'lr2': 2e-05,\n",
    " 'dt': 0.1,\n",
    " 'model': 'gln',\n",
    " 'batch_size': 68,\n",
    " 'l2reg': 5.7e-07,\n",
    "})\n",
    "# args = loaded['args']\n",
    "rng = jax.random.PRNGKey(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.ode import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HyperparameterSearch import new_get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfnc = jax.jit(jax.vmap(analytical_fn, 0, 0))\n",
    "vget = partial(jax.jit, backend='cpu')(jax.vmap(partial(get_trajectory_analytic, ), (0, None), 0))\n",
    "\n",
    "batch = 60\n",
    "\n",
    "@jax.jit\n",
    "def get_derivative_dataset(rng):\n",
    "    # randomly sample inputs\n",
    "\n",
    "    y0 = jnp.concatenate([\n",
    "        jax.random.uniform(rng, (batch, 2))*2.0*np.pi,\n",
    "        (jax.random.uniform(rng+1, (batch, 2))-0.5)*10*2\n",
    "    ], axis=1)\n",
    "    \n",
    "    return y0, vfnc(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = None\n",
    "best_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_random_params, nn_forward_fn = extended_mlp(args)\n",
    "import HyperparameterSearch\n",
    "HyperparameterSearch.nn_forward_fn = nn_forward_fn\n",
    "_, init_params = init_random_params(rng+1, (-1, 4))\n",
    "rng += 1\n",
    "model = (nn_forward_fn, init_params)\n",
    "opt_init, opt_update, get_params = optimizers.adam(args.lr)\n",
    "opt_state = opt_init(init_params)\n",
    "from jax.tree_util import tree_flatten\n",
    "from HyperparameterSearch import make_loss, train\n",
    "from copy import deepcopy as copy\n",
    "# train(args, model, data, rng);\n",
    "from jax.tree_util import tree_flatten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current std:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.ops import index_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HyperparameterSearch.nn_forward_fn = nn_forward_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's score the qdotdot output over normally distributed input for 256 batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = True\n",
    "n = 256\n",
    "\n",
    "@jax.jit\n",
    "def custom_init(stds, rng2):\n",
    "    new_params = []\n",
    "    i = 0\n",
    "    for l1 in init_params:\n",
    "        if (len(l1)) == 0: new_params.append(()); continue\n",
    "        new_l1 = []\n",
    "        for l2 in l1:\n",
    "            if len(l2.shape) == 1:\n",
    "                new_l1.append(jnp.zeros_like(l2))\n",
    "            else:\n",
    "                if normal:\n",
    "                    new_l1.append(jax.random.normal(rng2, l2.shape)*stds[i])\n",
    "#                     n1 = l2.shape[0]\n",
    "#                     n2 = l2.shape[1]\n",
    "#                     power = stds[0]\n",
    "#                     base_scale = stds[1]\n",
    "#                     s = base_scale/(n1+n2)**power\n",
    "#                     new_l1.append(jax.random.normal(rng2, l2.shape)*s)\n",
    "                else:\n",
    "                    new_l1.append(jax.random.uniform(rng2, l2.shape, minval=-0.5, maxval=0.5)*stds[i])\n",
    "                rng2+=1\n",
    "                i += 1\n",
    "\n",
    "        new_params.append(new_l1)\n",
    "        \n",
    "    return new_params\n",
    "\n",
    "@jax.jit\n",
    "def j_score_init(stds, rng2):\n",
    "    \n",
    "    new_params = custom_init(stds, rng2)\n",
    "    \n",
    "    rand_input = jax.random.normal(rng2, [n, 4])\n",
    "    rng2 += 1\n",
    "\n",
    "    outputs = jax.vmap(\n",
    "        partial(\n",
    "        raw_lagrangian_eom,\n",
    "        learned_dynamics(new_params)))(rand_input)[:, 2:]\n",
    "\n",
    "    #KL-divergence to mu=0, std=1:\n",
    "    mu = jnp.average(outputs, axis=0)\n",
    "    std = jnp.std(outputs, axis=0)\n",
    "    \n",
    "    KL = jnp.sum((mu**2 + std**2 - 1)/2.0  - jnp.log(std))\n",
    "    \n",
    "    \n",
    "    def total_output(p):\n",
    "        return vmap(partial(raw_lagrangian_eom, learned_dynamics(p)))(rand_input).sum()\n",
    "\n",
    "    d_params = grad(total_output)(new_params)\n",
    "    \n",
    "    for l1 in d_params:\n",
    "        if (len(l1)) == 0: continue\n",
    "        new_l1 = []\n",
    "        for l2 in l1:\n",
    "            if len(l2.shape) == 1: continue\n",
    "            \n",
    "            mu = jnp.average(l2)\n",
    "            std = jnp.std(l2)\n",
    "            KL += (mu**2 + std**2 - 1)/2.0 - jnp.log(std)\n",
    "            \n",
    "    #HACK\n",
    "#     KL += jnp.sum(stds**2)\n",
    "    return jnp.log10(KL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_std = jnp.array(\n",
    "[ 0.01]*(args.layers+1)\n",
    ")\n",
    "\n",
    "rng2 = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_score_init(cur_std, rng2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "\n",
    "vv = jax.jit(vmap(j_score_init, (None, 0), 0))\n",
    "\n",
    "rng2 = jax.random.PRNGKey(0)\n",
    "def score_init(stds):\n",
    "    global rng2\n",
    "    stds = jnp.array(stds)\n",
    "    stds = jnp.exp(stds)\n",
    "    q75, q50, q25 = np.percentile(vv(stds, jax.random.split(rng2, num=10)), [75, 50, 25])\n",
    "    rng2 += 30\n",
    "    \n",
    "\n",
    "    return q50, q75-q25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_init(cur_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bayes_opt import BayesianOptimization\n",
    "\n",
    "# # Bounded region of parameter space\n",
    "pbounds = {'s%d'%(i,): (-15, 15) for i in range(len(cur_std))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb(**kwargs):\n",
    "    out, std = score_init([kwargs[q] for q in ['s%d'%(i,) for i in range(len(cur_std))]])\n",
    "#     if out is None or not out > -30:\n",
    "#         return -30.0\n",
    "    return -out, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the best distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's redo that with Bayes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "\n",
    "\n",
    "def run_trial(args):\n",
    "    loss, std = bb(**args)\n",
    "    if loss == np.nan:\n",
    "        return {\n",
    "        'status': 'fail', # or 'fail' if nan loss\n",
    "        'loss': np.inf\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'status': 'ok', # or 'fail' if nan loss\n",
    "        'loss': -loss,\n",
    "        'loss_variance': std,\n",
    "    }\n",
    "\n",
    "\n",
    "#TODO: Declare your hyperparameter priors here:\n",
    "space = {\n",
    "    **{'s%d'%(i,): hp.normal('s%d'%(i,), -2, 5) for i in range(len(cur_std)-1)\n",
    "    },\n",
    "    **{'s%d'%(len(cur_std)-1,): hp.normal('s%d'%(len(cur_std)-1,), 3, 8)}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = fmin(run_trial,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5000,\n",
    "    trials=trials,\n",
    "    verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k(t):\n",
    "    if 'loss' not in t['result']:\n",
    "        return np.inf\n",
    "    return t['result']['loss']\n",
    "\n",
    "sorted_trials = sorted(trials.trials, key=k)\n",
    "len(trials.trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.array(\n",
    "    [[s['misc']['vals']['s%d'%(i,)][0] for i in range(len(cur_std))] for s in sorted_trials[:100]]\n",
    ")\n",
    "q[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 layers, 1000 hidden: {(4, 1000), (1000, 1000), (1000, 1000), (1000, 1)}\n",
    "\n",
    "## median top 10/2000: array([-1.47842217, -4.37217279, -3.37083752, 11.13480387])\n",
    "\n",
    "(unconverged)\n",
    "\n",
    "## 4 layers, 100 hidden:  {(4, 100), (100, 100), (100, 100), (100, 1)}\n",
    "\n",
    "## median top 30/5000: array([-1.70680816, -2.40340615, -2.17201716, 10.55268474])\n",
    "\n",
    "(unconverged)\n",
    "\n",
    "## 3 layers, 100 hidden:\n",
    "\n",
    "## median top 100/7000: array([-1.69875614, -2.74589338,  3.75818009])\n",
    "\n",
    "(unverged converged)\n",
    "\n",
    "## 3 layers, 30 hidden:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Eureqa to get the scalings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data = np.array(\n",
    "[\n",
    "    [t['misc']['vals']['s%d'%(i,)][0] for i in range(len(cur_std))] + [t['result']['loss']]\n",
    "for t in trials.trials if 'loss' in t['result'] and np.isfinite(t['result']['loss'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('sdata.npy', simple_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor, kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianProcessRegressor(alpha=3, n_restarts_optimizer=20, normalize_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data[:, -1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.fit(simple_data[:, :-1], simple_data[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.layers+1, args.hidden_dim, q[gp.predict(q).argmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New runs with noise added:\n",
    "\n",
    "## layers, hidden, log(std): (4, 30, array([-2.12770715, -1.99764457, -1.29472256,  6.1514019 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicted with GP for 3 layers, 100 hidden, {(4, 100), (100, 100), (100, 1)}\n",
    "\n",
    "array([-1.95669793, -2.39555616,  1.92755129])\n",
    "\n",
    "## predicted with GP for 3 layers, 50 hidden, {(4, 50), (50, 50), (50, 1)}\n",
    "\n",
    "array([-1.77223004, -3.2154843 , 10.38542243])\n",
    "\n",
    "\n",
    "## predicted with GP for 3 layers, 30 hidden:\n",
    "\n",
    "array([-1.47298021, -4.10931435,  2.60899782])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(simple_data[:, 0], simple_data[:, 1], c=simple_data[:, -1])\n",
    "# plt.ylim(-5, 2)\n",
    "# plt.xlim(-5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num= 50\n",
    "# x = np.linspace(-12, 2, num=num)\n",
    "# y = np.linspace(-12, 2, num=num)\n",
    "# X,Y = np.meshgrid(x, y) # grid of point\n",
    "# Z = gp.predict(np.stack((X.ravel(), Y.ravel()), axis=1)).reshape(*[num]*2) # evaluation of the function on the grid\n",
    "\n",
    "# im = plt.imshow(np.log10(Z), cmap='viridis', extent=[-12, 2, -12, 2], origin='lower')\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=bb,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.maximize(init_points=4**len(cur_std), n_iter=300+4**len(cur_std), alpha=1e-2, normalize_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l1 in init_params:\n",
    "    if (len(l1)) == 0: continue\n",
    "    for l2 in l1:\n",
    "        print(l2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data = np.array(\n",
    "[[t['params']['s%d'%(i,)] for i in range(len(cur_std))] + [t['target']]\n",
    "for t in optimizer.res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('hidden={}_layers={}_results.npy'.format(args.hidden_dim, args.layers), simple_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor, kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianProcessRegressor(alpha=1e-2, n_restarts_optimizer=20, normalize_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_data[:, -1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.fit(simple_data[:, :-1], simple_data[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data[:, :-1][gp.predict(simple_data[:, :-1]).argmin()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main2",
   "language": "python",
   "name": "main2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
